# 챗봇 만들기

- 지금까지 두 가지 문제에 대해 실습을 진행했다. 4장에서는 텍스트를 분석해서 각 텍스트를 분류하는 문제를 실습했고, 5장에서는 두 개의 텍스트가 있을 때 각 텍스트끼리의 유사도를 판단하는 문제를 실습했다. 이번 장에서는 텍스트를 단순히 분석해서 분류나 유사도를 측정하는 것이 아닌 직접 문장을 생성할 수 있는 텍스트 생성(text generation) 문제를 실습해 보겠다. 텍스트 생성에도 많은 문제가 있지만 '자연어의 꽃'이라고 불리는 '챗봇'을 제작해 본다.
- 일반적으로 챗봇을 제작하는 방법은 매우 다양하다. 단순하게 규칙 기반으로 제작할 수도 있고, 머신러닝을 활용한 유사도 기반, 규칙과 머신러닝을 섞은 하이브리드 기반, 특정 시나리오 기반까지 정의하는 사람에 따라 제작 방법이 매우 다양하다. 이 책에서는 이러한 제작 방법 중에서 딥러닝 모델을 통한 챗봇을 만들어 보겠다. 또한 챗봇을 만들기 위한 딥러닝 모델에도 여러 가지가 있지만 그중에서 번역 문제에서 이미 성능이 입증된 시퀀스 투 시퀀스(Sequence to sequence) 모델을 활용해 챗봇을 제작하는 법을 알아보고자 한다.
- 하지만 모든 딥러닝 모델이 그렇듯 데이터가 있어야 모델을 학습할 수 있다. 따라서 한글로 챗봇을 만들기 위한 데이터에 대해 먼저 알아본다.

## 데이터 소개

|----|----|
|데이터이름|Chatbot data|
|데이터 용도|한국어 챗봇 학습을 목적으로 사용한다.|
|데이터 권한|MIT 라이센스|
|데이터 출처|https://github.com/songys/Chatbot_data|

- 일반적으로 공개된 챗봇을 위한 한글 데이터는 거의 없다고 봐도 무방하다. 심지어 한글보다 많은 데이터가 공개돼 있는 영어에도 'Ubuntu Dialogue Corpus' 데이터를 제외하면 공개된 데이터가 없다고 볼 수 있다. 다행히 한글로도 챗봇을 만들어 볼 수 있게 데이터를 제작해서 공개해주신 분들이 있다. 여기서 사용할 데이터는 송영숙님이 번역 및 제공해 주신 'Chatbot_data_for_Korean v1.0' 데이터셋이다.
- 이 데이터는 총 11,876개의 데이터로 구성돼 있고, 각 데이터는 질문과 그에 대한 대답, 그리고 주제에 대한 라벨값을 가지고 있다. 이 라벨값은 3가지로 구성돼 있는데 0은 일상 대화를 나타내고, 1은 부정, 2는 긍정의 주제를 의미한다. 앞서 다뤘던 데이터에 비하면 적은 수의 데이터이지만 10,000개가 넘는 데이터이기 때문에 연구 및 공부하기에는 충분한 양이다. 따라서 이 데이터를 사용해 텍스트 생성 문제, 그중에서도 챗봇을 직접 만들어 보자.

## 데이터 분석
- 이번 절에서는 실습을 진행하는 구성이 다른 장과 조금 다르다. 이전 장까지는 데이터에 대한 분석과 전처리를 진행한 후 전처리한 데이터를 가지고 여러 가지 모델링을 해봤다면 이번 장에서는 데이터 분석을 우선적으로 진행한 후 데이터 전처리와 모델을 한 번에 만들 것이다. 데이터 분석을 통해 나온 결과를 활용해 전처리 모듈을 만들어 보겠다. 
- 우선 챗봇 데이터를 분석해서 데이터의 고유한 특징을 파악한 후 모델링 과정에서 고려해야 할 사항을 확인해보자. 전체적인 데이터 분석 과정은 이전에 진행했던 것과 유사하게 진행한다. 추가로 특정 질문을 했을 때 어떤 응답이 나올 수 있는지도 유추해 보자.
- 먼저 데이터 분석을 위해 데이터를 불러온다. 판다스 라이브러리를 사용해 데이터프레임 형태로 데이터를 불러오자.

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from collections import Counter

from konlpy.tag import Okt

from functools import reduce
from wordcloud import WordCloud

DATA_IN_PATH = './data_in/'

data = pd.read_csv(DATA_IN_PATH + 'ChatBotData.csv', encoding='utf-8')
```

- 데이터를 불러오는 방법은 이전 방법과 동일하다. 데이터의 구조를 확인해 보기 위해 head 함수를 사용해 데이터의 일부만 출력해 보자.

```python
print(data.head())
```

|--|---|---|---|
||Q|A|label|
|0|12시 땡!|하루가 또 가네요|0|
|1|1지망 학교 떨어졌어|위로해 드립니다.|0|
|2|3박4일 놀러가고 싶다|여행은 언제나 좋죠.|0|
|3|3박4일 정도 놀러가고 싶다|여행은 언제나 좋죠.|0|
|4|PPL 심하네|눈살이 찌푸려지죠.|0|

- 데이터는 앞서 말했던 것과 동일한 구조로 돼 있다. 각 데이터는 Q, A 값인 질문과 대답 텍스트를 가지고 있고 그에 대한 라벨값을 가지고 있다. 해당 데이터에서 라벨값은 0,1,2로 구성돼 있다. 이제 데이터를 좀 더 깊이 있게 분석해 보자.

### 문장 전체에 대한 분석

- 먼저 데이터의 길이를 분석한다. 질문과 답변 모두에 대해 길이를 분석하기 위해 두 데이터를 하나의 리스트로 만들자.

```python
sentences = list(data['Q']) + list(data['A'])
```

- 질문과 답변 문장을 위와 같이 하나의 리스트로 만들었다면 이제 길이를 분석한다. 이전 장까지 2개의 기준으로 분석을 진행했다. 문자 단위의 길이 분석과 단어의 길이 분석을 진행했는데, 이번 장에서는 하나의 기준을 추가해서 세 가지 기준으로 분석을 진행한다. 분석 기준은 다음과 같다.
    - 분자 단위의 길이 분석(음절)
    - 단어 단위의 길이 분석(어절)
    - 형태소 단위의 길이 분석
