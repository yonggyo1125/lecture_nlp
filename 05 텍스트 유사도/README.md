# 텍스트 유사도

- 이전 장에서는 텍스트 분류 문제 중에서 감정 분석 문제를 해결하기 위해 데이터를 전처리하고 여러 가지 모델링을 통해 성능을 측정했다. 그뿐만 아니라 한글과 영어 텍스트를 자연어 처리할 때 어떤 부분이 서로 다른지 알아보기 위해 영어 텍스트 분류와 한글 텍스트 분류로 두 가지로 나눠서 알아봤다. 
- 이번 장에서는 자연어 처리의 또 다른 문제인 텍스트 유사도 문제를 해결해 보겠다. 텍스트 유사도 문제란 두 문장(글)이 있을 때 두 문장 간의 유사도를 측정할 수 있는 모델을 만드는 것이다. 텍스트 유사도에 대해서는 3장에 자세히 나와 있으므로 참고하길 바란다. 
- 이번 장에서도 캐글의 대회 중 하나를 해결해 보려고 한다. "Quora Questions Pairs"라는 문제가 이번 장에서 해결할 문제인데, 쿼라(Quora)는 질문을 하고 다른 사용자들로부터 답변을 받을 수 있는 서비스로서 이 서비스에 올라온 여러 질문들 중에서 어떤 질문이 서로 유사한지 판단하는 모델을 만드는 것이 이번 장의 목표다. 
- 이번에는 영어 텍스트와 한글 텍스트를 모두 다루지는 않을 것이다. 쿼라 영어 데이터를 가지고 모델링하고, 한글 데이터를 통해 텍스트 유사도를 측정하는 실습은 진행하지 않는다. 4장에서 진행했던 한글 데이터 처리를 생각해 보면 한글 데이터에 대해서도 텍스트 유사도를 측정하는 것이 어렵지 않을 것이다. 
- 먼저 이번 장에서 다룰 문제와 해당 데이터에 대해 자세히 알아보자.

## 문제 소개

|||
|---|----|
|데이터 이름|Quora Question Pairs|
|데이터 권한|쿼라 권한을 가지고 있으며 캐글 가입 후 데이터를 내려받으면 문제없다.|
|데이터 출처|https://www.kaggle.com/quora-question-pairs/data|

- 쿼라(Quora)는 앞서 설명했듯이 질문과 답변을 할 수 있는 사이트다. 실제로 딥러닝에 대해 공부할 떄도 쿼라의 질문들을 참고하면서 많은 공부를 할 수 있다. 쿼라의 월 사용자는 대략 1억명 정도 된다. 매일 수많은 질문들이 사이트에 올라올 텐데 이 많은 질문 중에는 분명히 중복된 것들이 포함될 것이다. 따라서 쿼라 입장에서는 중복된 질문들을 잘 찾기만 한다면 이미 잘 작성된 답변들을 사용자들이 참고하게 할 수 있고, 더 좋은 서비스를 제공할 수 있게 된다. 
- 참고로 현재 쿼라에서는 이미 중복에 대한 검사를 하고 있다. 앞서 배운 랜덤 포레스트 모델을 통해 중복 질문들을 찾고 있다. 
- 이번 장의 내용은 이전 장과 비슷하게 진행된다. 우선 데이터에 대해 간단히 알아본 후 데이터를 자세히 분석하고 그 결과를 토대로 데이터 전처리를 할 것이다. 이후에는 전처리된 데이터를 활용해 여러 가지 모델링을 진행하고 모델들을 비교하면서 이번 장을 마무리할 것이다. 먼저 데이터에 대해 알아보자.

## 데이터 분석과 전처리

- 데이터를 가지고 모델링 하기 위해서는 데이터에 대한 분석과 전처리를 진행해야 한다. 데이터 분석을 통해 데이터의 특징을 파악하고 이를 바탕으로 데이터 전처리 작업을 진행한다. 여기서는 주로 문장의 길이와 어휘 빈도 문석을 해서 그 결과를 전처리에 적용하고자 한다. 데이터를 내려 받는 것부터 시작해서 데이터를 분석한 후 전처리하는 순서로 진행할 것이다. 4장에서 다룬 내용과 유사하므로 큰 어려움 없이 진행할 수 있을 것이다. 

### 데이터 불러오기와 분석하기
- 이번에 다룰 문제는 앞서 소개한 것처럼 캐글의 대회 중 하나인 "Quora Question Pairs"다. 먼저 해당 데이터를 내려받는 것부터 시작한다. 캐글 문제의 데이터를 내려받으려면 앞서 2장에서 설명했던 것처럼 직접 대회 페이지에서 다운로드 하거나 캐글 API를 사용해서 다운로드할 수 있다. 직접 다운로드하는 경우 캐글에 로그인해 "Quora Question Pairs" 대회에 들어가 대회 규정에 동의한 후 데이터 탭에서 다운로드하면 되고, API를 통해 다운로드하는 경우 명령행을 열고 다음 명령을 입력해서 내려받으면 된다. 

```
kaggle competitions download -C quora-question-pairs
```

- 캐글에서 다운로드한 파일의 압축을 풀고, 아래 3개의 파일을 `data_in` 폴더로 이동한다.
    - sample_submission.csv.zip
    - test.csv.zip
    - train.csv.zip
- 위 3개의 파일을 `data_in` 폴더로 이동시켰다면, 이제 데이터 분석을 시작하도록 하자. 우선 데이터를 분석하기 위한 패키지를 모두 불러온다. 

```python
import zipfile

DATA_IN_PATH = './data_in/'

file_list = ['train.csv.zip', 'test.csv.zip', 'sample_submission.csv.zip']

for file in file_list:
    zipRef = zipfile.ZipFile(DATA_IN_PATH + file, 'r')
    zipRef.extractall(DATA_IN_PATH)
    zipRef.close()
```

```python
import numpy as np 
import pandas as pd
import os
import matplotlib.pyplot as plt
import seaborn as sns
```

- 보다시피 넘파이, 판다스, 시각화를 위한 라이브러리인 맷플롯립과 시본을 비롯해 기본 내장 패키지인 `os`를 불러왔다. 모두 4장에서 사용했던 라이브러리이며, 자세한 설명은 2장에 나와있다. 
- 학습 데이터를 불러와서 어떤 형태로 데이터가 구성돼 있는지 확인해 보자. 판다스의 데이터 프레임 형태로 불러온다. 

```python
train_data = pd.read_csv(DATA_IN_PATH + 'train.csv')
train_data.head()
```

![스크린샷 2025-04-01 오후 10 03 14](https://github.com/user-attachments/assets/dd12dbe4-90bb-451e-becb-1c21fd615f64)




